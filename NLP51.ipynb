{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGgs2L7/3slSRxD5NViQm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raviteja-kota/NLP-Repo-A051/blob/main/NLP51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746f5fdc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Adzzs-1Ozxw"
      },
      "outputs": [],
      "source": [
        "#Tokenization\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"Good Afternoon Boss\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sent}\")\n",
        "\n",
        "# Word Tokenization\n",
        "print(\"\\nWord Tokenization:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    words = word_tokenize(sent)\n",
        "    print(f\"Sentence {i} Words: {words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"The king will never knell their head hanging on their feet\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(text)\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "id": "JFUtCB9FRIAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"running runs runner easily cats\"\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(text)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "id": "wd-CkePSUnvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Morphology\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"running runs runner easily cats\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stem and Lemmatize each word\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Stemmed Words:\", stemmed)\n",
        "print(\"Lemmatized Words:\", lemmatized)\n"
      ],
      "metadata": {
        "id": "3Ua_vDYkVT8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "\n",
        "import string\n",
        "\n",
        "text = \"Hello! This is NLP Normalization, isn't it cool?\"\n",
        "\n",
        "# Lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Strip whitespace\n",
        "text = text.strip()\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "yJnra-rjYpp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spell correction\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Input text with spelling mistakes\n",
        "text = \"I havv a speling errror in this sentnce.\"\n",
        "\n",
        "# Correct the spelling\n",
        "corrected_text = TextBlob(text).correct()\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Corrected:\", corrected_text)\n"
      ],
      "metadata": {
        "id": "Sn0a0_erzHvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deduction\n",
        "\n",
        "facts = [\"All humans are mortal\", \"Socrates is a human\"]\n",
        "query = \"Is Socrates mortal?\"\n",
        "\n",
        "if \"Socrates\" in facts[1] and \"humans are mortal\" in facts[0]:\n",
        "    print(\"Yes, Socrates is mortal.\")\n",
        "else:\n",
        "    print(\"Cannot deduce.\")\n"
      ],
      "metadata": {
        "id": "2DBK1RZa2JIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unigram\n",
        "from collections import Counter\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Create the n-grams\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "\n",
        "    # Join the n-grams into a tuple\n",
        "    return [' '.join(gram) for gram in ngrams]\n",
        "\n",
        "# Example usage\n",
        "text = \"I love programming in Python\"\n",
        "n = 2  # Bigram example\n",
        "bigrams = generate_ngrams(text, n)\n",
        "\n",
        "# Print the bigrams\n",
        "for bigram in bigrams:\n",
        "    print(bigram)\n"
      ],
      "metadata": {
        "id": "_tb2JkL-aej3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uni & Bigram\n",
        "def generate_ngrams(text, n):\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Create the n-grams using zip\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "\n",
        "    # Join the n-grams into a string and return\n",
        "    return [' '.join(gram) for gram in ngrams]\n",
        "\n",
        "# Example usage\n",
        "text = \"I love programming in Python\"\n",
        "\n",
        "# Generate unigrams (n = 1)\n",
        "unigrams = generate_ngrams(text, 1)\n",
        "\n",
        "# Generate bigrams (n = 2)\n",
        "bigrams = generate_ngrams(text, 2)\n",
        "trigarms = generate_ngrams(text, 3)\n",
        "\n",
        "# Print the results\n",
        "print(\"Unigrams:\")\n",
        "for unigram in unigrams:\n",
        "    print(unigram)\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "for bigram in bigrams:\n",
        "    print(bigram)\n",
        "print(\"\\nTrigrams:\")\n",
        "for trigram in trigarms:\n",
        "    print(trigram)"
      ],
      "metadata": {
        "id": "81uh0eWGaev3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word prediction\n",
        "\n",
        "\n",
        "import nltk\n",
        "from collections import Counter, defaultdict\n",
        "nltk.download('reuters'); nltk.download('punkt')\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Prepare corpus\n",
        "corpus = [[w.lower() for w in sent] for sent in reuters.sents()]\n",
        "\n",
        "# Build counts\n",
        "unigram = Counter(); bigram = defaultdict(Counter); vocab = set()\n",
        "for sent in corpus:\n",
        "    for i, w in enumerate(sent):\n",
        "        unigram[w] += 1; vocab.add(w)\n",
        "        if i < len(sent)-1: bigram[w][sent[i+1]] += 1\n",
        "\n",
        "V = len(vocab)\n",
        "\n",
        "# Predict next word with Laplace smoothing\n",
        "def predict_next(word):\n",
        "    return sorted({w: (bigram[word][w]+1)/(unigram[word]+V) for w in vocab}.items(),\n",
        "                  key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "# Real-time loop\n",
        "while True:\n",
        "    s = input(\"\\nType a sentence (or 'exit'): \").lower()\n",
        "    if s==\"exit\": break\n",
        "    w = s.split()[-1]\n",
        "    print(f\"Next word predictions after '{w}': {predict_next(w)}\")\n"
      ],
      "metadata": {
        "id": "iEZECmF3m5_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ngram smoothened +1 method\n",
        "from collections import Counter\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return [' '.join(gram) for gram in ngrams]\n",
        "\n",
        "def add_one_smoothing(ngrams):\n",
        "    counts = Counter(ngrams)\n",
        "    vocabulary = set(ngrams)\n",
        "    smoothed_counts = {ngram: count + 1 for ngram, count in counts.items()}\n",
        "\n",
        "    total_count = sum(smoothed_counts.values()) + (len(vocabulary) - len(smoothed_counts))\n",
        "\n",
        "    probs = {ngram: count / total_count for ngram, count in smoothed_counts.items()}\n",
        "\n",
        "    return probs\n",
        "\n",
        "# Example usage\n",
        "text = \"I love programming in Python\"\n",
        "n = 2\n",
        "\n",
        "ngrams = generate_ngrams(text, n)\n",
        "smoothed_probs = add_one_smoothing(ngrams)\n",
        "\n",
        "print(\"Ngrams and their smoothed probabilities:\")\n",
        "for ngram, prob in smoothed_probs.items():\n",
        "    print(f\"{ngram}: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "83ChYgHNae3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word generation\n",
        "\n",
        "import random\n",
        "\n",
        "# Sample text (training data)\n",
        "text = \"I love natural language processing and I love learning NLP\"\n",
        "\n",
        "# Step 1: Split text into words\n",
        "words = text.split()\n",
        "\n",
        "# Step 2: Build a dictionary of word pairs\n",
        "# Example: {\"I\": [\"love\"], \"love\": [\"natural\", \"learning\"], ...}\n",
        "word_dict = {}\n",
        "for i in range(len(words) - 1):\n",
        "    word = words[i]\n",
        "    next_word = words[i + 1]\n",
        "    if word not in word_dict:\n",
        "        word_dict[word] = []\n",
        "    word_dict[word].append(next_word)\n",
        "\n",
        "# Step 3: Choose a random starting word\n",
        "word = random.choice(words)\n",
        "\n",
        "# Step 4: Generate new words\n",
        "sentence = [word]\n",
        "for i in range(10): # generate 10 words\n",
        "    if word in word_dict:\n",
        "        next_word = random.choice(word_dict[word])\n",
        "        sentence.append(next_word)\n",
        "        word = next_word\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Step 5: Print generated sentence\n",
        "print(\"Generated sentence:\")\n",
        "print(\" \".join(sentence))"
      ],
      "metadata": {
        "id": "ODhjDAYJafA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS TAGGING IN NLTK\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Input text\n",
        "text = \"No other power is more powerfull than the power primate.Chorooo!.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Display results\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"POS Tags:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:10} --> {tag}\")\n"
      ],
      "metadata": {
        "id": "W464iQAAmWSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bending pos tagg\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"Python is a powerful programming language.\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "nQs7tLR5DITU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hidden MArkov model\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "train_data = [\n",
        "    [('The', 'DT'), ('dog', 'NN'), ('saw', 'VB'), ('a', 'DT'), ('cat', 'NN')],\n",
        "    [('The', 'DT'), ('cat', 'NN'), ('sat', 'VB'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')],\n",
        "    [('A', 'DT'), ('dog', 'NN'), ('slept', 'VB')],\n",
        "    [('The', 'DT'), ('big', 'JJ'), ('dog', 'NN'), ('barked', 'VB')]\n",
        "]\n",
        "\n",
        "print(\"--- Training Data ---\")\n",
        "for sent in train_data:\n",
        "    print(sent)\n",
        "print(\"\\n\")\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "\n",
        "tagger = trainer.train(train_data)\n",
        "\n",
        "print(\"--- Model Training Complete ---\")\n",
        "print(f\"Trained on {len(train_data)} sentences.\\n\")\n",
        "\n",
        "print(\"--- Tagging New Sentences ---\")\n",
        "\n",
        "sentence1 = ['The', 'cat', 'saw', 'the', 'dog']\n",
        "tags1 = tagger.tag(sentence1)\n",
        "print(f\"Sentence: {sentence1}\")\n",
        "print(f\"Predicted Tags: {tags1}\\n\")\n",
        "\n",
        "sentence2 = ['The', 'dog', 'chased', 'the', 'cat']\n",
        "tags2 = tagger.tag(sentence2)\n",
        "print(f\"Sentence: {sentence2}\")\n",
        "print(f\"Predicted Tags: {tags2}\\n\")\n",
        "\n",
        "sentence3 = ['A', 'flooby', 'dog', 'slept']\n",
        "tags3 = tagger.tag(sentence3)\n",
        "print(f\"Sentence: {sentence3}\")\n",
        "print(f\"Predicted Tags: {tags3}\\n\")"
      ],
      "metadata": {
        "id": "NLOtvm9w3bPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hmm\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# 1. Define Training Data\n",
        "train_data = [\n",
        "    [('The', 'DT'), ('dog', 'NN'), ('saw', 'VB'), ('a', 'DT'), ('cat', 'NN')],\n",
        "    [('The', 'DT'), ('cat', 'NN'), ('sat', 'VB'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')],\n",
        "    [('A', 'DT'), ('dog', 'NN'), ('slept', 'VB')],\n",
        "    [('The', 'DT'), ('big', 'JJ'), ('dog', 'NN'), ('barked', 'VB')]\n",
        "]\n",
        "\n",
        "# 2. Train the HMM Tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(train_data)\n",
        "\n",
        "print(\"--- Tagger Trained ---\")\n",
        "\n",
        "# 3. Test the Tagger\n",
        "test_sentences = [\n",
        "    ['The', 'cat', 'saw', 'the', 'dog'],\n",
        "    ['The', 'dog', 'chased', 'the', 'cat'], # 'chased' is unknown\n",
        "    ['A', 'flooby', 'dog', 'slept']      # 'flooby' is unknown\n",
        "]\n",
        "\n",
        "# Loop through test sentences and print results\n",
        "for sent in test_sentences:\n",
        "    print(f\"\\nInput: {sent}\")\n",
        "    print(f\"Tagged: {tagger.tag(sent)}\")"
      ],
      "metadata": {
        "id": "A04Y-pHuh3tk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}